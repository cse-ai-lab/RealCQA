- run eval_PMC.py, with all the weight files in one directory, to get the detailed score about predictions with respect to ground truth, of all the categories of question and the overall score.


s2.npy 
s3.npy
s4.npy
s5.npy

t_id_map.npy  -- taxonomy aggeregate - generates eval for per root/structure/retrieval/reasoning
ctype_id_map.npy -- chart type aggregate - generates eval for per chart type Line/VBar/Hbar/Scatter/VBox